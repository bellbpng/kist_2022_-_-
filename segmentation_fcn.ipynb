{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ./disk1/colonoscopy_datasetv2/cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 지정 & 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def readData(self):\n",
    "        all_files = []\n",
    "        all_labels = []\n",
    "\n",
    "        img_files = os.walk(self.data_path).__next__()[2]\n",
    "\n",
    "        for img in img_files:\n",
    "            img_path = os.path.join(self.data_path, img)\n",
    "            image = Image.open(img_path)\n",
    "            if image is not None:\n",
    "                if img[4:8] == 'MASK':\n",
    "                    all_labels.append(img_path)\n",
    "                else:\n",
    "                    all_files.append(img_path)\n",
    "\n",
    "        # 오름차순 정렬\n",
    "        all_files.sort()\n",
    "        all_labels.sort()\n",
    "\n",
    "        return all_files, all_labels, len(all_files), len(all_labels)\n",
    "\n",
    "    def __init__(self, data_path, img_transforms=None, mask_transforms=None):\n",
    "        self.data_path = data_path\n",
    "        self.img_transforms = img_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        self.img_files, self.img_masks, self.data_size, self.mask_size = self.readData()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.img_files[index]\n",
    "        mask = self.img_masks[index]\n",
    "\n",
    "        image = Image.open(image)\n",
    "        mask = Image.open(mask)\n",
    "        if self.img_transforms is not None:\n",
    "            image = self.img_transforms(image)\n",
    "        \n",
    "        if self.mask_transforms is not None:\n",
    "            mask = self.mask_transforms(mask)\n",
    "\n",
    "        return {'image':image, 'mask':mask}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 불러오기 & 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), \n",
    "                        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "mask_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "total_dataset = CustomDataset('./ADC/', data_transforms, mask_transforms)\n",
    "print('data size: {}'.format(total_dataset.data_size))\n",
    "print('mask size: {}'.format(total_dataset.mask_size))\n",
    "\n",
    "to_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(8,20))\n",
    "cnt = 1\n",
    "for idx, item in enumerate(total_dataset):\n",
    "    if idx==5: \n",
    "        break\n",
    "    sample_image = to_image(total_dataset[idx]['image'])\n",
    "    sample_mask = to_image(total_dataset[idx]['mask'])\n",
    "\n",
    "    plt.subplot(5,2,cnt)\n",
    "    plt.title('{}'.format(total_dataset.img_files[idx]))\n",
    "    plt.imshow(sample_image)\n",
    "    plt.subplot(5,2,cnt+1)\n",
    "    plt.title('{}'.format(total_dataset.img_masks[idx]))\n",
    "    plt.imshow(sample_mask)\n",
    "    cnt+=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 구현\n",
    "def train_epoch(dataloader, model, optimizer):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    crit = nn.BCELoss()\n",
    "   \n",
    "    for item in dataloader:\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['mask'].to(device)\n",
    "\n",
    "        aux_out = model(images)['aux']\n",
    "        aux_out = sigmoid(aux_out)\n",
    "        outputs = model(images)['out']\n",
    "        outputs = sigmoid(outputs)\n",
    "        aux_loss = crit(aux_out, labels)\n",
    "        main_loss = crit(outputs, labels)\n",
    "        loss = main_loss*0.9 + aux_loss*0.1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item() * len(images)\n",
    "\n",
    "    # print('loss: {}'.format(loss.item()))\n",
    "    # print('outputs: {}'.format(outputs[0]))\n",
    "    # print('labels: {}'.format(labels[0]))\n",
    "    \n",
    "    return sum_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate_epoch(dataloader, model, optimizer):\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    crit = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            images = item['image'].to(device)\n",
    "            labels = item['mask'].to(device)\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            outputs = sigmoid(outputs)\n",
    "            loss = crit(outputs, labels)\n",
    "\n",
    "            sum_loss += loss.item() * len(images)\n",
    "\n",
    "    return sum_loss / len(dataloader.dataset)\n",
    "\n",
    "def inference(dataloader, PATH, model_n, model_st, model_all):\n",
    "    model = torch.load(PATH + model_n)\n",
    "    model.load_state_dict(torch.load(PATH + model_st))\n",
    "    checkpoint = torch.load(PATH+model_all)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        iou_score = 0\n",
    "        cnt = 0\n",
    "    for item in dataloader:\n",
    "        cnt += 1\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['mask'].to(device)     \n",
    "\n",
    "        outputs = model(images)['out']\n",
    "        outputs = sigmoid(outputs)\n",
    "        outputs[outputs<threshold] = 0\n",
    "        outputs[outputs>threshold] = 1\n",
    "\n",
    "\n",
    "        # find pixel accuracy\n",
    "        total += 224.*224*len(images)\n",
    "        result = torch.eq(outputs,labels)\n",
    "        result = result.view(-1) # 1차원 배열로 만들기\n",
    "        for val in result:\n",
    "            if val == True:\n",
    "                correct += 1\n",
    "\n",
    "        # Mean Intersection-over-Union\n",
    "        intersection = torch.logical_and(labels,outputs)\n",
    "        union = torch.logical_or(labels,outputs)\n",
    "        iou_score += torch.sum(intersection) / torch.sum(union)\n",
    "    \n",
    "    # print('total: {}, correct: {}'.format(total, correct))\n",
    "    iou_score = iou_score/cnt\n",
    "    pixel_accuracy = float(correct/total) * 100\n",
    "    print('Test pixel accuracy of the model on the {} test images: {}%'.format(len(dataloader.dataset), pixel_accuracy))\n",
    "    print('IOU score of the model on the {} test images: {}'.format(len(dataloader.dataset), iou_score))\n",
    "\n",
    "    # 시각화\n",
    "    to_image = transforms.ToPILImage()\n",
    "\n",
    "    plt.figure(figsize=(8,12))\n",
    "    fig = 1\n",
    "\n",
    "    for i in range(3):\n",
    "        out = to_image(outputs[i])\n",
    "        # img = to_image(images[i])\n",
    "        mask = to_image(labels[i])\n",
    "        #plt.subplot(3,2,fig)\n",
    "        # plt.imshow(img)\n",
    "        # plt.title('image')\n",
    "        plt.subplot(3,2,fig)\n",
    "        plt.imshow(out)\n",
    "        plt.title('predict')\n",
    "        plt.subplot(3,2,fig+1)\n",
    "        plt.imshow(mask)\n",
    "        plt.title('mask')\n",
    "        fig+=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(total_dataset.data_size*0.8)\n",
    "validation_size = int(total_dataset.data_size*0.1)\n",
    "test_size = total_dataset.data_size - train_size - validation_size\n",
    "\n",
    "print('train size: {}'.format(train_size))\n",
    "print('validation size: {}'.format(validation_size))\n",
    "print('test size: {}'.format(test_size))\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(total_dataset, [train_size, validation_size, test_size],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCN_Resenet50 모델 불러오기 & 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)\n",
    "\n",
    "#fine tuning\n",
    "num_classes = 1\n",
    "num_ftrs1 = model.aux_classifier[4].in_channels\n",
    "model.aux_classifier[4] = nn.Conv2d(num_ftrs1,num_classes,kernel_size=(1,1),stride=(1,1))\n",
    "num_ftrs2 = model.classifier[4].in_channels\n",
    "model.classifier[4] = nn.Conv2d(num_ftrs2,num_classes,kernel_size=(1,1),stride=(1,1))\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "#하이퍼파라미터 설정\n",
    "hy_batch = 32\n",
    "hy_lr = 0.00001\n",
    "hy_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../home/bokyoungk/segmentation_models/resnet50/'\n",
    "train_loader = DataLoader(train_dataset, batch_size=hy_batch, shuffle=False)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=hy_batch, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hy_batch, shuffle=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=hy_lr)\n",
    "\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "min_loss = 0.2765420353412628\n",
    "\n",
    "for e in range(0,hy_epoch):\n",
    "    print('----------------------epoch {}--------------------------'.format(e+1))\n",
    "    train_loss = train_epoch(train_loader,model,optimizer)\n",
    "    val_loss = validate_epoch(val_loader,model,optimizer)\n",
    "    print('train loss: {}'.format(train_loss))\n",
    "    print('val loss: {}'.format(val_loss))\n",
    "    all_train_loss.append(train_loss)\n",
    "    all_val_loss.append(val_loss)\n",
    "\n",
    "    # 모델 저장\n",
    "    if min_loss > val_loss:\n",
    "        min_loss = val_loss\n",
    "        torch.save(model, PATH + 'min_model_{}th.pt'.format(e+1)) # 전체 모델 저장\n",
    "        torch.save(model.state_dict(), PATH + 'min_model_state_dict_{}th.pt'.format(e+1))\n",
    "        torch.save({\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict()\n",
    "        }, PATH+'min_all_{}th.tar'.format(e+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loss 그래프 그리기\n",
    "x = np.arange(1,hy_epoch+1,step=1)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,all_train_loss,label='train loss')\n",
    "plt.plot(x,all_val_loss,label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../home/bokyoungk/segmentation_models/resnet50/'\n",
    "model_n = 'min_model_16th.pt'\n",
    "model_st = 'min_model_state_dict_16th.pt'\n",
    "model_all = 'min_all_16th.tar'\n",
    "test_loader = DataLoader(test_dataset, batch_size=hy_batch, shuffle=True)\n",
    "# Inference\n",
    "inference(test_loader, PATH, model_n, model_st, model_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭에 따른 변화 관찰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=hy_batch, shuffle=False)\n",
    "\n",
    "PATH = '../../../home/bokyoungk/segmentation_models/resnet50/'\n",
    "model_n = 'min_model_28th.pt'\n",
    "model_st = 'min_model_state_dict_28th.pt'\n",
    "model_all = 'min_all_28th.tar'\n",
    "\n",
    "model = torch.load(PATH + model_n)\n",
    "model.load_state_dict(torch.load(PATH + model_st))\n",
    "checkpoint = torch.load(PATH+model_all)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "model.eval()\n",
    "threshold = 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for item in test_loader:\n",
    "        origins = item['image'].to(device)\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['mask'].to(device)\n",
    "        outputs = model(images)['out']\n",
    "        outputs[outputs<threshold] = 0\n",
    "        outputs[outputs>threshold] = 1\n",
    "        break\n",
    "\n",
    "# 특정 이미지만 출력\n",
    "to_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.title('epoch 28 predict')\n",
    "output = to_image(outputs[2])\n",
    "plt.imshow(output)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.title('mask image')\n",
    "label = to_image(labels[2])\n",
    "plt.imshow(label)\n",
    "\n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.title('image')\n",
    "# origin = to_image(origins[1])\n",
    "# plt.imshow(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
