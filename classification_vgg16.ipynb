{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../../disk1/colonoscopy_datasetv2/cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 지정 & 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def readImg(self):\n",
    "        all_img_files = []\n",
    "\n",
    "        class_names = os.walk(self.dataset_path).__next__()[1]\n",
    "\n",
    "        for idx, class_name in enumerate(class_names):\n",
    "            img_dir = os.path.join(self.dataset_path, class_name)\n",
    "            img_files = os.walk(img_dir).__next__()[2]\n",
    "            \n",
    "            for img in img_files:\n",
    "                if img[4:8] == 'MASK':\n",
    "                    continue\n",
    "                img_path = os.path.join(img_dir,img)\n",
    "                image = Image.open(img_path)\n",
    "                if image is not None:\n",
    "                    all_img_files.append(img_path)\n",
    "\n",
    "        all_img_files.sort()\n",
    "\n",
    "        return all_img_files, len(class_names), len(all_img_files)\n",
    "\n",
    "    def __init__(self, dataset_path, img_transforms=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.img_transforms = img_transforms\n",
    "        self.img_files, self.num_classes, self.num_images = self.readImg()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = None\n",
    "        image = self.img_files[index]\n",
    "        if image[2:5]==\"NOR\":\n",
    "            label = 0\n",
    "        elif image[2:5]==\"ADC\":\n",
    "            label = 1\n",
    "        elif image[2:5]==\"HGD\":\n",
    "            label = 2\n",
    "        elif image[2:5]==\"LGD\":\n",
    "            label = 3\n",
    "            \n",
    "        image = Image.open(image)\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        \n",
    "        if self.img_transforms is not None:\n",
    "            image = self.img_transforms(image)\n",
    "\n",
    "\n",
    "        return {'image':image, 'label':label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss,self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1-pt)**self.gamma*ce_loss).mean()\n",
    "        \n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    crit = FocalLoss()\n",
    "    sum_loss = 0\n",
    "\n",
    "    for item in dataloader:\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = crit(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()*len(images)\n",
    "\n",
    "    return sum_loss / len(dataloader.dataset)\n",
    "\n",
    "def val_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    crit = FocalLoss()\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    total = len(dataloader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            images = item['image'].to(device)\n",
    "            labels = item['label'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = crit(outputs, labels)\n",
    "\n",
    "            sum_loss += loss.item()*len(images)\n",
    "            _, predict = torch.max(outputs.data,1)\n",
    "            correct += (predict==labels).sum().item()\n",
    "\n",
    "        accuracy = correct/total * 100\n",
    "\n",
    "        return sum_loss/len(dataloader.dataset), accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "total_dataset = CustomDataset('./', img_transforms=data_transforms)\n",
    "total_loader = DataLoader(total_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# total images: 653, total classes: 4\n",
    "print('total images: {}'.format(total_dataset.num_images))\n",
    "print('total classes: {}'.format(total_dataset.num_classes))\n",
    "\n",
    "# class별 데이터 개수 확인\n",
    "# nor0, adc1, hgd2, lgd3 = 0, 0, 0, 0\n",
    "# for idx, item in enumerate(total_dataset):\n",
    "#     label = item['label']\n",
    "#     if label==0: nor0 += 1\n",
    "#     elif label==1: adc1 += 1\n",
    "#     elif label==2: hgd2 += 1\n",
    "#     else: lgd3 += 1\n",
    "# print(\"nor: {}\\nadc: {}\\nhgd: {}\\nlgd: {}\".format(nor0,adc1,hgd2,lgd3))\n",
    "\n",
    "train_size = int(total_dataset.num_images * 0.8)\n",
    "val_size = int(total_dataset.num_images * 0.1)\n",
    "test_size = total_dataset.num_images - train_size - val_size\n",
    "print('train size: {}\\nvalidation_size: {}\\ntest_size: {}'.format(train_size, val_size, test_size))\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(total_dataset, [train_size, val_size, test_size],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 모델 불러오기 & 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "pre_model = models.vgg16(pretrained=True)\n",
    "# scr_model = models.vgg16(pretrained=False)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "hy_batch = 64\n",
    "hy_epoch = 100\n",
    "hy_lr = 0.00001\n",
    "\n",
    "# fine tuning\n",
    "num_classes = total_dataset.num_classes\n",
    "num_ftrs = pre_model.classifier[6].in_features\n",
    "\n",
    "pre_model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "pre_model.cuda()\n",
    "pre_model = nn.DataParallel(pre_model).to(device)\n",
    "# scr_model.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "# scr_model.cuda()\n",
    "# scr_model = nn.DataParallel(scr_model).to(device)\n",
    "\n",
    "optimizer_pre = torch.optim.Adam(pre_model.parameters(), lr=hy_lr)\n",
    "# optimizer_scr = torch.optim.Adam(scr_model.parameters(), lr=hy_lr)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=hy_batch, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hy_batch, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hy_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_pre = '../../../home/bokyoungk/classification_models/vgg16_pretrained/'\n",
    "PATH_scr = '../../../home/bokyoungk/classification_models/vgg16_scratch/'\n",
    "# pre_model = torch.load(PATH_pre+'checkpoint/model_11.pt')\n",
    "# pre_model.load_state_dict(torch.load(PATH_pre+'checkpoint/model_state_11.pt'))\n",
    "# checkpoint = torch.load(PATH_pre+'checkpoint/all_11.tar')\n",
    "# pre_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# scr_model = torch.load(PATH_scr+'checkpoint/model_11.pt')\n",
    "# scr_model.load_state_dict(torch.load(PATH_scr+'checkpoint/model_state_11.pt'))\n",
    "# checkpoint = torch.load(PATH_scr+'checkpoint/all_11.tar')\n",
    "# scr_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "min_loss_pre = 1.2\n",
    "# min_loss_scr = 1.2\n",
    "\n",
    "all_train_loss_pre = []\n",
    "all_val_loss_pre = []\n",
    "all_accuracy_pre = []\n",
    "# all_train_loss_scr = []\n",
    "# all_val_loss_scr = []\n",
    "# all_accuracy_scr = []\n",
    "\n",
    "\n",
    "for e in range(0,hy_epoch):\n",
    "    print('------------------------------------------------epoch {}/{}---------------------------------------------------'.format(e+1,hy_epoch))\n",
    "    train_loss_pre = train_epoch(pre_model,train_loader,optimizer_pre)\n",
    "    val_loss_pre, val_acc_pre = val_epoch(pre_model,val_loader)\n",
    "    # train_loss_scr = train_epoch(scr_model,train_loader,optimizer_scr)\n",
    "    # val_loss_scr, val_acc_scr = val_epoch(scr_model,val_loader)\n",
    "    print('train loss pretrained: {}, val loss pretrained: {}, val acc pretrained: {}'.format(train_loss_pre,val_loss_pre,val_acc_pre))\n",
    "    # print('train loss scratch: {}, val loss scratch: {}, val acc scratch: {}'.format(train_loss_scr,val_loss_scr,val_acc_scr))\n",
    "\n",
    "    all_train_loss_pre.append(train_loss_pre)\n",
    "    all_val_loss_pre.append(val_loss_pre)\n",
    "    all_accuracy_pre.append(val_acc_pre)\n",
    "\n",
    "    # all_train_loss_scr.append(train_loss_scr)\n",
    "    # all_val_loss_scr.append(val_loss_scr)\n",
    "    # all_accuracy_scr.append(val_acc_scr)\n",
    "\n",
    "    # # loss 최소일 때 저장\n",
    "    if min_loss_pre > val_loss_pre:\n",
    "        min_loss_pre = val_loss_pre\n",
    "        torch.save(pre_model,PATH_pre+'min_loss/focal_min_model_{}.pt'.format(e+1))\n",
    "        torch.save(pre_model.state_dict(),PATH_pre+'min_loss/focal_min_model_state_{}.pt'.format(e+1))\n",
    "        torch.save({\n",
    "            'model':pre_model.state_dict(),\n",
    "            'optimizer':optimizer_pre.state_dict()\n",
    "        },PATH_pre+'min_loss/focal_min_all_{}.tar'.format(e+1))\n",
    "\n",
    "    # if min_loss_scr > val_loss_scr:\n",
    "    #     min_loss_scr = val_loss_scr\n",
    "    #     torch.save(scr_model,PATH_scr+'min_loss/cr_min_model_{}.pt'.format(e+1))\n",
    "    #     torch.save(scr_model.state_dict(),PATH_scr+'min_loss/cr_min_model_state_{}.pt'.format(e+1))\n",
    "    #     torch.save({\n",
    "    #         'model':scr_model.state_dict(),\n",
    "    #         'optimizer':optimizer_scr.state_dict()\n",
    "    #     },PATH_scr+'min_loss/cr_min_all_{}.tar'.format(e+1))\n",
    "\n",
    "    \n",
    "    # checkpoint\n",
    "    # if (e+1)%10==0:\n",
    "    #     torch.save(pre_model,PATH_pre+'checkpoint/model_{}.pt'.format(e+1))\n",
    "    #     torch.save(pre_model.state_dict(),PATH_pre+'checkpoint/model_state_{}.pt'.format(e+1))\n",
    "    #     torch.save({\n",
    "    #         'model':pre_model.state_dict(),\n",
    "    #         'optimizer':optimizer_pre.state_dict()\n",
    "    #     },PATH_pre+'checkpoint/all_{}.tar'.format(e+1))\n",
    "\n",
    "    # if (e+1)%10==0:\n",
    "    #     torch.save(scr_model,PATH_scr+'checkpoint/focal_model_{}.pt'.format(e+1))\n",
    "    #     torch.save(scr_model.state_dict(),PATH_scr+'checkpoint/focal_model_state_{}.pt'.format(e+1))\n",
    "    #     torch.save({\n",
    "    #         'model':scr_model.state_dict(),\n",
    "    #         'optimizer':optimizer_scr.state_dict()\n",
    "    #     },PATH_scr+'checkpoint/focal_all_{}.tar'.format(e+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "x = np.arange(1,hy_epoch+1,step=1)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Pretrained-VGG16')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,all_train_loss_pre,label='train loss')\n",
    "plt.plot(x,all_val_loss_pre,label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Pretrained-VGG16')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "plt.plot(x,all_accuracy_pre)\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.title('Scratch-VGG16')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.plot(x,all_train_loss_scr,label='train loss')\n",
    "# plt.plot(x,all_val_loss_scr,label='val loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.title('Scratch-VGG16')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy(%)')\n",
    "# plt.plot(x,all_accuracy_scr)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_pre = '../../../home/bokyoungk/classification_models/vgg16_pretrained/'\n",
    "PATH_scr = '../../../home/bokyoungk/classification_models/vgg16_scratch/'\n",
    "\n",
    "#inference\n",
    "true_labels = []\n",
    "pre_labels = []\n",
    "# scr_labels = []\n",
    "\n",
    "pre_model = torch.load(PATH_pre+'min_loss/focal_min_model_10.pt')\n",
    "pre_model.load_state_dict(torch.load(PATH_pre+'min_loss/focal_min_model_state_10.pt'))\n",
    "checkpoint = torch.load(PATH_pre+'min_loss/focal_min_all_10.tar')\n",
    "pre_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# scr_model = torch.load(PATH_scr+'min_loss/cr_min_model_35.pt')\n",
    "# scr_model.load_state_dict(torch.load(PATH_scr+'min_loss/cr_min_model_state_35.pt'))\n",
    "# checkpoint = torch.load(PATH_scr+'min_loss/cr_min_all_35.tar')\n",
    "# scr_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    pre_model.eval()\n",
    "    # scr_model.eval()\n",
    "\n",
    "    correct_pre = 0\n",
    "    # correct_scr = 0\n",
    "    total = len(test_loader.dataset)\n",
    "    for item in test_loader:\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "\n",
    "        outputs_pre = pre_model(images)\n",
    "        # outputs_scr = scr_model(images)\n",
    "        _, predict_pre = torch.max(outputs_pre,1)\n",
    "        # _, predict_scr = torch.max(outputs_scr,1)\n",
    "        correct_pre += (predict_pre==labels).sum().item()\n",
    "        # correct_scr += (predict_scr==labels).sum().item()\n",
    "        true_labels.extend(labels)\n",
    "        pre_labels.extend(predict_pre)\n",
    "        # scr_labels.extend(predict_scr)\n",
    "    \n",
    "    print('Test accuracy of the pre-trained VGG16 on the {} test images: {}%'.format(total, 100*correct_pre/total))\n",
    "    # print('Test accuracy of the scratch VGG16 on the {} test images: {}%'.format(total, 100*correct_scr/total))\n",
    "\n",
    "true_labels = torch.tensor(true_labels)\n",
    "true_labels = true_labels.tolist()\n",
    "pre_labels = torch.tensor(pre_labels)\n",
    "pre_labels = pre_labels.tolist()\n",
    "# scr_labels = torch.tensor(scr_labels)\n",
    "# scr_labels = scr_labels.tolist()\n",
    "\n",
    "print('Pre-trained VGG16')\n",
    "print(classification_report(true_labels,pre_labels))\n",
    "# print('Scratch VGG16')\n",
    "# print(classification_report(true_labels,scr_labels))\n",
    "\n",
    "print('Pre-trained VGG16')\n",
    "print(confusion_matrix(true_labels,pre_labels))\n",
    "# print('Scratch VGG16')\n",
    "# print(confusion_matrix(true_labels,scr_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
