{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ./disk1/colonoscopy_datasetv2/cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 지정 & 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def readData(self):\n",
    "        all_files = []\n",
    "        all_labels = []\n",
    "\n",
    "        img_files = os.walk(self.data_path).__next__()[2]\n",
    "\n",
    "        for img in img_files:\n",
    "            img_path = os.path.join(self.data_path, img)\n",
    "            image = Image.open(img_path)\n",
    "            if image is not None:\n",
    "                if img[4:8] == 'MASK':\n",
    "                    all_labels.append(img_path)\n",
    "                else:\n",
    "                    all_files.append(img_path)\n",
    "\n",
    "        # 오름차순 정렬\n",
    "        all_files.sort()\n",
    "        all_labels.sort()\n",
    "\n",
    "        return all_files, all_labels, len(all_files), len(all_labels)\n",
    "\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.img_files, self.img_masks, self.data_size, self.mask_size = self.readData()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.img_files[index]\n",
    "        mask = self.img_masks[index]\n",
    "\n",
    "        image = Image.open(image)\n",
    "        mask = Image.open(mask)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "\n",
    "        return {'image':image, 'mask':mask}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 불러오기 & 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "check_data_transforms = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])\n",
    "total_dataset = CustomDataset('./ADC/', check_data_transforms)\n",
    "print('data size: {}'.format(total_dataset.data_size))\n",
    "print('mask size: {}'.format(total_dataset.mask_size))\n",
    "\n",
    "to_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(8,20))\n",
    "cnt = 1\n",
    "for idx, item in enumerate(total_dataset):\n",
    "    if idx==5: \n",
    "        break\n",
    "    sample_image = to_image(total_dataset[idx]['image'])\n",
    "    sample_mask = to_image(total_dataset[idx]['mask'])\n",
    "\n",
    "    plt.subplot(5,2,cnt)\n",
    "    plt.title('{}'.format(total_dataset.img_files[idx]))\n",
    "    plt.imshow(sample_image)\n",
    "    plt.subplot(5,2,cnt+1)\n",
    "    plt.title('{}'.format(total_dataset.img_masks[idx]))\n",
    "    plt.imshow(sample_mask)\n",
    "    cnt+=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('img shape: {}'.format(total_dataset[4]['image'].shape))\n",
    "print('mask shape: {}'.format(total_dataset[4]['mask'].shape))\n",
    "print('check img value: {}'.format(total_dataset[4]['image']))\n",
    "print('check mask value: {}'.format(total_dataset[4]['mask']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Network 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Network 구현\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(SimpleNet, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # encoder\n",
    "        self.enc_1 = self.conv_module(3,32)\n",
    "        self.enc_2 = nn.Dropout2d(0.2)\n",
    "        self.enc_3 = self.conv_module(32,32) # conv1\n",
    "        self.enc_4 = nn.MaxPool2d(kernel_size=2) \n",
    "        self.enc_5 = self.conv_module(32,64)\n",
    "        self.enc_6 = nn.Dropout2d(0.2)\n",
    "        self.enc_7 = self.conv_module(64,64) # conv2\n",
    "        self.enc_8 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # decoder\n",
    "        self.dec_1 = self.conv_module(64,128)\n",
    "        self.dec_2 = nn.Dropout2d(0.2)\n",
    "        self.dec_3 = self.conv_module(128,64) # conv3\n",
    "        self.dec_4 = self.conv_module(128,64) # concat upsampling(conv3) + conv2\n",
    "        self.dec_5 = nn.Dropout2d(0.2)\n",
    "        self.dec_6 = self.conv_module(64,32) # conv4\n",
    "        self.dec_7 = self.conv_module(64,32) # concat upsampling(conv4) + conv1 \n",
    "        self.dec_8 = nn.Dropout2d(0.2)\n",
    "        self.dec_9 = self.conv_module(32,32) # conv5\n",
    "\n",
    "        self.output = nn.Conv2d(32, num_classes, kernel_size=1, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def conv_module(self, in_num, out_num):\n",
    "        layer = nn.Sequential(nn.Conv2d(in_num, out_num, kernel_size=3, padding=1),\n",
    "                            nn.ReLU())\n",
    "        \n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.enc_1(x)\n",
    "        out = self.enc_2(out)\n",
    "        conv1 = self.enc_3(out)\n",
    "        out = self.enc_4(conv1)\n",
    "        out = self.enc_5(out)\n",
    "        out = self.enc_6(out)\n",
    "        conv2 = self.enc_7(out)\n",
    "        out = self.enc_8(conv2)\n",
    "\n",
    "        out = self.dec_1(out)\n",
    "        out = self.dec_2(out)\n",
    "        conv3 = self.dec_3(out)\n",
    "        up1 = nn.UpsamplingNearest2d(scale_factor=(2,2))(conv3)\n",
    "        out = torch.cat((up1,conv2), dim=1)\n",
    "        out = self.dec_4(out)\n",
    "        out = self.dec_5(out)\n",
    "        conv4 = self.dec_6(out)\n",
    "        up2 = nn.UpsamplingNearest2d(scale_factor=(2,2))(conv4)\n",
    "        out = torch.cat((up2, conv1), dim=1)\n",
    "        out = self.dec_7(out)\n",
    "        out = self.dec_8(out)\n",
    "        out = self.dec_9(out)\n",
    "        out = self.output(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# model summary\n",
    "model = SimpleNet()\n",
    "model.to(device)\n",
    "summary(model, (3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모듈 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 구현\n",
    "from torch import logical_and\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, model, optimizer):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    crit = nn.BCELoss()\n",
    "   \n",
    "\n",
    "    for item in dataloader:\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['mask'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = crit(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item() * len(images)\n",
    "\n",
    "    # print('loss: {}'.format(loss.item()))\n",
    "    # print('outputs: {}'.format(outputs[0]))\n",
    "    # print('labels: {}'.format(labels[0]))\n",
    "    \n",
    "    return sum_loss / len(dataloader.dataset)\n",
    "\n",
    "def validate_epoch(dataloader, model, optimizer):\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    crit = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            images = item['image'].to(device)\n",
    "            labels = item['mask'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = crit(outputs, labels)\n",
    "\n",
    "            sum_loss += loss.item() * len(images)\n",
    "\n",
    "    return sum_loss / len(dataloader.dataset)\n",
    "\n",
    "def inference(dataloader, PATH, model_n, model_st, model_all):\n",
    "    model = torch.load(PATH + model_n)\n",
    "    model.load_state_dict(torch.load(PATH + model_st))\n",
    "    checkpoint = torch.load(PATH+model_all)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        iou_score = 0\n",
    "        cnt = 0\n",
    "    for item in dataloader:\n",
    "        cnt += 1\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['mask'].to(device)     \n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs[outputs<threshold] = 0\n",
    "        outputs[outputs>threshold] = 1\n",
    "\n",
    "        # find pixel accuracy\n",
    "        total += 256.*256*len(images)\n",
    "        result = torch.eq(outputs,labels)\n",
    "        result = result.view(-1) # 1차원 배열로 만들기\n",
    "        for val in result:\n",
    "            if val == True:\n",
    "                correct += 1\n",
    "\n",
    "        # Mean Intersection-over-Union\n",
    "        intersection = torch.logical_and(labels,outputs)\n",
    "        union = torch.logical_or(labels,outputs)\n",
    "        iou_score += torch.sum(intersection) / torch.sum(union)\n",
    "    \n",
    "    # print('total: {}, correct: {}'.format(total, correct))\n",
    "    iou_score = iou_score/cnt\n",
    "    pixel_accuracy = float(correct/total) * 100\n",
    "    print('Test pixel accuracy of the model on the {} test images: {}%'.format(len(dataloader.dataset), pixel_accuracy))\n",
    "    print('IOU score of the model on the {} test images: {}'.format(len(dataloader.dataset), iou_score))\n",
    "\n",
    "    # 시각화\n",
    "    to_image = transforms.ToPILImage()\n",
    "\n",
    "    plt.figure(figsize=(8,12))\n",
    "    fig = 1\n",
    "\n",
    "    for i in range(3):\n",
    "        out = to_image(outputs[i])\n",
    "        img = to_image(images[i])\n",
    "        mask = to_image(labels[i])\n",
    "        plt.subplot(3,3,fig)\n",
    "        plt.imshow(img)\n",
    "        plt.title('image')\n",
    "        plt.subplot(3,3,fig+1)\n",
    "        plt.imshow(out)\n",
    "        plt.title('predict')\n",
    "        plt.subplot(3,3,fig+2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title('mask')\n",
    "        fig+=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(total_dataset.data_size*0.8)\n",
    "validation_size = int(total_dataset.data_size*0.1)\n",
    "test_size = total_dataset.data_size - train_size - validation_size\n",
    "\n",
    "print('train size: {}'.format(train_size))\n",
    "print('validation size: {}'.format(validation_size))\n",
    "print('test size: {}'.format(test_size))\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(total_dataset, [train_size, validation_size, test_size],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hy_batch = 32\n",
    "hy_lr = 0.00001\n",
    "hy_epoch = 80\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=hy_batch, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=hy_batch, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hy_batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "PATH = '../../../home/bokyoungk/segmentation_models/net1/'\n",
    "\n",
    "# model_n = 'min_model_98th.pt'\n",
    "# model_st = 'min_model_state_dict_98th.pt'\n",
    "# model_all = 'min_all_98th.tar'\n",
    "\n",
    "# model = torch.load(PATH+model_n)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=hy_lr)\n",
    "# model.load_state_dict(torch.load(PATH+model_st))\n",
    "\n",
    "# checkpoint = torch.load(PATH+model_all)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hy_lr)\n",
    "# Training\n",
    "min_loss = 0.510586462020874\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "for e in range(0,hy_epoch):\n",
    "    print('---------------------------epoch {}-------------------------------'.format(e+1))\n",
    "    train_loss = train_epoch(train_loader,model,optimizer)\n",
    "    validation_loss = validate_epoch(validation_loader,model,optimizer)\n",
    "    print('train loss= {}'.format(train_loss))\n",
    "    print('validation loss= {}'.format(validation_loss))\n",
    "    all_train_loss.append(train_loss)\n",
    "    all_val_loss.append(validation_loss)\n",
    "\n",
    "\n",
    "    # 모델 저장\n",
    "    # if min_loss > validation_loss:\n",
    "    #     min_loss = validation_loss\n",
    "    #     torch.save(model, PATH + 'min_model_{}th.pt'.format(e+1)) # 전체 모델 저장\n",
    "    #     torch.save(model.state_dict(), PATH + 'min_model_state_dict_{}th.pt'.format(e+1))\n",
    "    #     torch.save({\n",
    "    #         'model':model.state_dict(),\n",
    "    #         'optimizer':optimizer.state_dict()\n",
    "    #     }, PATH+'min_all_{}th.tar'.format(e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# loss 그래프 그리기\n",
    "x = np.arange(1,hy_epoch+1,step=1)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(x,all_train_loss,label='train loss')\n",
    "plt.plot(x,all_val_loss,label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../home/bokyoungk/segmentation_models/net1/'\n",
    "model_n = 'min_model_69th.pt'\n",
    "model_st = 'min_model_state_dict_69th.pt'\n",
    "model_all = 'min_all_69th.tar'\n",
    "\n",
    "# Inference\n",
    "inference(test_loader, PATH, model_n, model_st, model_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭에 따른 변화 관찰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../home/bokyoungk/segmentation_models/net1/'\n",
    "model_n = 'min_model_144th.pt'\n",
    "model_st = 'min_model_state_dict_144th.pt'\n",
    "model_all = 'min_all_144th.tar'\n",
    "\n",
    "model = torch.load(PATH + model_n)\n",
    "model.load_state_dict(torch.load(PATH + model_st))\n",
    "checkpoint = torch.load(PATH+model_all)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "model.eval()\n",
    "threshold = 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for item in test_loader:\n",
    "        origins = item['image'].to(device)\n",
    "        images = item['image'].to(device)\n",
    "        labels = item['mask'].to(device)\n",
    "        outputs = model(images)\n",
    "        outputs[outputs<threshold] = 0\n",
    "        outputs[outputs>threshold] = 1\n",
    "        break\n",
    "\n",
    "# 특정 이미지만 출력\n",
    "to_image = transforms.ToPILImage()\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.title('epoch 144 predict')\n",
    "output = to_image(outputs[3])\n",
    "plt.imshow(output)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.title('mask image')\n",
    "label = to_image(labels[3])\n",
    "plt.imshow(label)\n",
    "\n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.title('image')\n",
    "# origin = to_image(origins[1])\n",
    "# plt.imshow(origin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
